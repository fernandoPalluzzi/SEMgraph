% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/semLearn.R
\name{SEMtree}
\alias{SEMtree}
\title{Tree-based structure learning methods}
\usage{
SEMtree(graph, data, seed, type = "ST", alpha = 0.05, verbose = FALSE, ...)
}
\arguments{
\item{graph}{An igraph object.}

\item{data}{A matrix or data.frame. Rows correspond to subjects, and
columns to graph nodes (variables).}

\item{seed}{A vector of seed nodes.}

\item{type}{Tree-based structure learning method. Four algorithms 
are available:
\itemize{
\item "ST" (default). Steiner Tree (ST) identification via fast Kou's algorithm 
(1981) connecting a set of seed nodes (called Terminal vertices) with connector
nodes (called Steiner vertices) from input graph as defined in \code{graph}
with minimal total distance on its edges. 
\item "MST". Minimum Spanning Tree (MST) identification via Prim's algorithm
(Prim, 1957). The latter finds the subset of edges that includes every vertex
of the graph (as defined in \code{graph}) such that the sum of the weights 
of the edges can be minimized. The argument \code{seed} is
set to NULL (i.e., no seed nodes are needed).
\item "CAT". Causal additive trees (CAT) algorithm as in Jakobsen et al. 
(2022). While the previous algorithms rely on the input graph, the "CAT" 
algorithm is data-driven. The argument \code{graph} is set to NULL 
(i.e., no input graph is needed). In the first step, a (univariate) generalized
additive model (GAM) is employed to estimate the conditional
expectations E[X_{i}|X_{j} = x] for all i != j, then use these to construct edge
weights as inputs to the Chu–Liu–Edmonds’ algorithm (Chow and Liu, 1968).
Argument \code{seed} must be specified to analyse a subset of nodes (variables)
of interest.
\item "CPDAG". CLE algorithm for Skeleton Recovery and CPDAG
estimation as in Lou et al. (2021). Together with "CAT" algorithm, "CPDAG" is 
data-driven and the argument \code{graph} is set to NULL.
The key idea is to first recover the skeleton of the polytree by applying 
the CLE algorithm  to the pairwise sample correlations of the data matrix.
After the skeleton is recovered, the set of all v-structures can be correctly
identified via a simple thresholding approach to pairwise sample correlations.
Finally, the CPDAG of the polytree can be found applying iteratively only
Rule 1 of Meek (1995). Argument \code{seed} must be specified to analyse a
subset of nodes (variables) of interest.}}

\item{alpha}{Threshold for rejecting a pair of node being independent in 
"CPDAG" algorithm. The latter implements a natural v-structure identification 
procedure by thresholding the pairwise sample correlations over all adjacent 
pairs of edges with some appropriate threshold. By default, 
\code{alpha = 0.05}.}

\item{verbose}{If TRUE, it shows the output tree (not recommended for large graphs).}

\item{...}{Currently ignored.}
}
\value{
An \code{igraph} object. If \code{type = "ST"}, seed nodes are 
colored in green and connectors in white. If \code{type = "ST"} and
\code{type = "MST"}, edges are colored in green if not present in the input
graph. If \code{type = "CPDAG"}, bidirected edges are colored in golden
(if the algorithm is not able to establish the direction of the relationship
between x and y).
}
\description{
Four tree-based structure learning methods are implemented
with graph and data-driven algorithms. The graph methods refer to the 
fast Steiner Tree (ST) Kou's algorithm, and the identification of 
the Minimum Spanning Tree (MST) with Prim's algorithm. 
The data-driven methods propose fast and scalable procedures based on 
Chow-Liu–Edmonds’ algorithm (CLE) to recover the skeleton of the
polytree. The first method, called Causal Additive Trees (CAT) uses
pairwise addittive weights as input for CLE algorithm. The second one
applies CHE algorithm for skeleton recovery and extends the skeleton to
a Completed Partially Directed Acyclic Graph (CPDAG).
}
\details{
If the input graph is a directed graph, ST and MST undirected trees are
converted in directed trees using the \code{\link[SEMgraph]{orientEdges}} function.
If the input graph is an undirected graph, ST and MST undirected trees are 
converted in a directed polytree using CAT algorithm with (univariate) linear
regression for conditional expectation mapped on the output tree.
}
\examples{

# Sachs data
data <- log(sachs$pkc)
graph <- sachs$graph

# graph-based trees
seed <- V(graph)$name[sample(1:10, 5)]
tree1<- SEMtree(graph, data, seed=seed, type="ST", verbose=TRUE)
tree2<- SEMtree(graph, data, seed=NULL, type="MST", verbose=TRUE)

# data-driven trees
V <- colnames(data)[colnames(data) \%in\% V(graph)$name]
tree3<- SEMtree(NULL, data, seed=V, type="CAT", verbose=TRUE)
tree4<- SEMtree(NULL, data, seed=V, type="CPDAG", alpha=0.05, verbose=TRUE)

}
\references{
Kou, L., Markowsky, G., Berman, L. (1981). A fast algorithm for Steiner trees. 
Acta Informatica 15, 141–145. <https://doi.org/10.1007/BF00288961>

Prim, R.C. (1957). Shortest connection networks and some generalizations Bell
System Technical Journal, 37 1389–1401. 

Chow, C.K. and Liu, C. (1968). Approximating discrete probability distributions with 
dependence trees. IEEE Transactions on Information Theory, 14(3):462–467.

Meek, C. (1995). Causal inference and causal explanation with background knowledge.
In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,
403–410.

Jakobsen, M, Shah, R., Bühlmann, P., Peters, J. (2022). 
Structure Learning for Directed Trees. arXiv:
<https://doi.org/10.48550/arxiv.2108.08871>.

Lou, X., Hu, Y., Li, X. (2022). Linear Polytree Structural Equation Models:
Structural Learning and Inverse Correlation Estimation. arXiv:
<https://doi.org/10.48550/arxiv.2107.10955>
}
\author{
Mario Grassi \email{mario.grassi@unipv.it}
}
